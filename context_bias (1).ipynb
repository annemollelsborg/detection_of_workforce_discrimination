{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Step 1 â€” Load CSV\n",
    "# Columns: \"Job Title\" and \"Job Description\"\n",
    "df = pd.read_csv(\"job_descriptions.csv\")\n",
    "\n",
    "# Example bias terms and job description, should be replaced\n",
    "bias_terms = [\"dominerende\", \"aggressiv\", \"rockstjerne\", \"ungdommelig\", \"naturlig leder\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias detection system using contextual embedding + similarity\n",
    "\n",
    "After applying the lexicon-based bias detection algorithm, it is valuable to enhance the detection process using a semantic model. By embedding both the sentences of the job descriptions and the predefined bias terms, we can leverage semantic similarity to identify potential bias in job descriptions. This is achieved by computing the cosine similarity between the sentence embeddings and the bias term embeddings.\n",
    "\n",
    "For each sentence in a job description, the cosine similarity score is compared to a pre-defined threshold. If the score exceeds this threshold, the sentence is labeled as \"potentially biased.\" The threshold is adjustable based on experimentation and evaluation, allowing flexibility in detecting bias at different sensitivity levels.\n",
    "\n",
    "However, it's important to note a few limitations of this approach:\n",
    "\n",
    "* Lack of Contextual Sensitivity: The model does not understand the context in which the bias terms are used. For example, if a job description mentions \"young people,\" and the bias term list contains the word \"youthful,\" the sentence will be flagged as biased based solely on the similarity to the term \"youthful\". This does not account for whether \"youthful\" refers to actual youth, or if it is used in a different context entirely.\n",
    "\n",
    "* False Positives: Since the function compares word embeddings directly, there is a possibility that sentences may be wrongly flagged as biased even if they do not express discriminatory intent. For example, a sentence may contain the word \"leader\" in a neutral context, but if the bias term \"natural leader\" is part of the bias lexicon, the sentence might still be flagged. This emphasizes that the model cannot capture all nuances in language and context.\n",
    "\n",
    "* Manual Verification: As a result of the above limitations, any sentence flagged as potentially biased should be manually reviewed. It is essential to determine whether the flagged sentence truly reflects a biased statement or if the word appears in a different context that is not discriminatory. The functionâ€™s role is to highlight potential bias; it cannot make final judgments regarding the presence of bias without human oversight.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_analysis(df, bias_terms, title_col=\"title\", desc_col=\"description\", threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detects potential bias in job descriptions based on semantic similarity to bias terms.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): A pandas DataFrame with columns \"title\" and \"description\".\n",
    "        bias_terms (list of str): List of bias words/phrases to detect.\n",
    "        threshold (float): Cosine similarity threshold to flag potential bias.\n",
    "\n",
    "    Returns:\n",
    "        results (list of dict): Each dict contains job title, biased sentence, matched terms, and score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load sentence-transformer model\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Precompute embeddings for bias terms\n",
    "    bias_embeddings = model.encode(bias_terms, convert_to_tensor=True)\n",
    "\n",
    "    # Loop through each job description\n",
    "    for idx, row in df.iterrows():\n",
    "        title = row[\"title\"]\n",
    "        description = row[\"description\"]\n",
    "\n",
    "        # Skip missing descriptions\n",
    "        if pd.isna(description) or description.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        # Split into sentences\n",
    "        all_sentences = sent_tokenize(description)\n",
    "\n",
    "        # Embed all sentences\n",
    "        sentence_embeddings = model.encode(all_sentences, convert_to_tensor=True)\n",
    "\n",
    "        # Compute cosine similarities: (num_sentences x num_bias_terms)\n",
    "        cosine_scores = util.cos_sim(sentence_embeddings, bias_embeddings)\n",
    "\n",
    "        # Check each sentence separately\n",
    "        for i, sentence in enumerate(all_sentences):\n",
    "            # Skip sentences too short to be meaningful\n",
    "            if len(sentence.split()) < 5 or len(sentence) < 30:\n",
    "                continue\n",
    "            \n",
    "            # Get the cosine similarity scores for the current sentence\n",
    "            scores = cosine_scores[i]\n",
    "\n",
    "            # Check if any score exceeds the threshold\n",
    "            matched_indices = (scores > threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            # If there are any matches, store the results\n",
    "            if len(matched_indices) > 0:\n",
    "                matched_terms = [bias_terms[j] for j in matched_indices]\n",
    "                results.append({\n",
    "                    \"Job Title\": title,\n",
    "                    \"Potential Bias in\": sentence,\n",
    "                    \"Matched terms\": matched_terms,\n",
    "                    \"Max Score\": torch.max(scores).item()\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Job Title: To juridiske chefer med strategisk overblik og med lyst til\n",
      "  - Potential Bias in: 'Erfaring med ledelse â€“ gerne strategisk ledelse og evne til at omsÃ¦tte strategi til handling.'\n",
      "  - Matched terms: ['naturlig leder']\n",
      "  - Max Score: 0.5431661009788513\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Underviser sÃ¸ges til ZBC10 i NÃ¦stved\n",
      "  - Potential Bias in: 'Vi tilbyder et job med: Mulighed for at gÃ¸re en forskel for unge mennesker.'\n",
      "  - Matched terms: ['ungdommelig']\n",
      "  - Max Score: 0.5460309982299805\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Erfaren sygeplejerske med lyst til nattevagt\n",
      "  - Potential Bias in: 'En nÃ¦rvÃ¦rende og tilgÃ¦ngelig leder der har fokus pÃ¥ din og fÃ¦llesskabets trivsel og udvikling.'\n",
      "  - Matched terms: ['naturlig leder']\n",
      "  - Max Score: 0.595171332359314\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Motiveres du af relationsarbejde med unge? Bliv\n",
      "  - Potential Bias in: 'Velkommen indenfor pÃ¥ afsnit B203 Vi er et Ã¥bent ungdomsafsnit, der har plads til i alt 11 dÃ¸gnindlagte patienter i alderen 14-17 Ã¥r.'\n",
      "  - Matched terms: ['ungdommelig']\n",
      "  - Max Score: 0.5448185205459595\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Motiveres du af relationsarbejde med unge? Bliv\n",
      "  - Potential Bias in: 'I samarbejde med den unge er din opgave som kontaktperson at stÃ¸tte og strukturere den unges dagligdag, samt dokumentere relevante observationer dagligt.'\n",
      "  - Matched terms: ['ungdommelig']\n",
      "  - Max Score: 0.5355962514877319\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Sygeplejerske til Ungdomspsykiatrisk Ambulatorium ved BÃ¸rne- og\n",
      "  - Potential Bias in: 'Om Ungdomspsykiatrisk Ambulatorium B195 I Ambulatoriet for Unge udredes og behandles unge i alderen 14 - 17 Ã¥r, som er henvist for psykisk sygdom eller psykisk udviklingsforstyrrelse af et omfang, der krÃ¦ver udredning og eventuel behandling i hospitalsregi.'\n",
      "  - Matched terms: ['ungdommelig']\n",
      "  - Max Score: 0.5298155546188354\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Motiveres du af relationsarbejde med unge? Bliv\n",
      "  - Potential Bias in: 'AnsÃ¸g pÃ¥ virksomhedens side Fuldtid 2100 KÃ¸benhavn Motiveres du af relationsarbejde med unge?'\n",
      "  - Matched terms: ['ungdommelig']\n",
      "  - Max Score: 0.5164312124252319\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Motiveres du af relationsarbejde med unge? Bliv\n",
      "  - Potential Bias in: 'Velkommen indenfor pÃ¥ afsnit B203 Vi er et Ã¥bent ungdomsafsnit, der har plads til i alt 11 dÃ¸gnindlagte patienter i alderen 14-17 Ã¥r.'\n",
      "  - Matched terms: ['ungdommelig']\n",
      "  - Max Score: 0.5448185205459595\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Motiveres du af relationsarbejde med unge? Bliv\n",
      "  - Potential Bias in: 'I samarbejde med den unge er din opgave som kontaktperson at stÃ¸tte og strukturere den unges dagligdag, samt dokumentere relevante observationer dagligt.'\n",
      "  - Matched terms: ['ungdommelig']\n",
      "  - Max Score: 0.5355962514877319\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Kom og leg med os Bliv en del af vores fantastiske bÃ¸rnehus\n",
      "  - Potential Bias in: 'Medbestemmelse: SmÃ¥ grupper, hvor bÃ¸rnene deltager aktivt.'\n",
      "  - Matched terms: ['ungdommelig']\n",
      "  - Max Score: 0.5637026429176331\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: RÃ¥dgiver med interesse for global handel og told? Vi har brug\n",
      "  - Potential Bias in: 'Din leder sÃ¸rger for, at du lÃ¸bende fÃ¥r opgaver, der bÃ¥de udfordrer dig og giver dig mulighed for selvstÃ¦ndighed i opgavelÃ¸sningen.'\n",
      "  - Matched terms: ['naturlig leder']\n",
      "  - Max Score: 0.5076215267181396\n",
      "--------------------------------------------------\n",
      "ðŸ”¹ Job Title: Kvalitetsbevidste kollegaer sÃ¸ges til Kvalitetskontrol\n",
      "  - Potential Bias in: 'Ordentlige over for hinanden, over for vores leverandÃ¸rer og over for naturen.'\n",
      "  - Matched terms: ['naturlig leder']\n",
      "  - Max Score: 0.5100794434547424\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = context_analysis(df, bias_terms, title_col=\"Job Title\", desc_col=\"Job Description\")\n",
    "# Print results\n",
    "print(\"Results of Contextual Embedding and Similarity for Bias Detection:\")\n",
    "for job in results:\n",
    "    print(f\"ðŸ”¹ Job Title: {job['Job Title']}\")\n",
    "    if job['Potential Bias in']:\n",
    "        print(f\"  - Potential Bias in: '{job['Potential Bias in']}'\")\n",
    "        print(f\"  - Matched terms: {job['Matched terms']}\")\n",
    "        print(f\"  - Max Score: {job['Max Score']}\")\n",
    "    else:\n",
    "        print(\"  - No biased words found.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-level Bias Classifier\n",
    "After identifying sentences in job descriptions that are labeled as biased based on the bias terms both directly using lexicon-based detection and semantically using the contextual embedding, the next step is to build a classifier that can recognize biased language even when specific bias terms or their synonyms do not appear directly in the text.\n",
    "\n",
    "The primary goal of this classifier is to generalize beyond the predefined bias terms and detect subtle instances of bias that may not be captured by a lexicon-based approach. While lexicon-based detection rely on matching explicit words or phrases, a classifier can learn to recognize patterns and relationships in the language that indicate bias, regardless of whether the exact terms are used. This generalization ability allows the classifier to identify indirect forms of bias that might otherwise go unnoticed.\n",
    "\n",
    "By using a logistic regression model, we can train it on the labeled sentences (biased vs. non-biased) and enable it to learn the features and relationships between the text and the bias labels. The advantage of using this model is that it can process a wider range of language, recognizing biased language that does not rely solely on a pre-defined set of bias terms, and thus improving the overall detection accuracy.\n",
    "\n",
    "The goal of the classifier is to train it to recognize bias in sentences, even if these sentences do not explicitly contain pre-defined bias terms. By achieving this, the classifier will be able to determine if any sentences in a job description is potentially bias. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example bias terms and job description, should be replaced\n",
    "bias_terms_weak_label = [\"dominerende\", \"aggressiv\", \"ekstrovert\", \"ansvarlig\", \"naturlig leder\", \"faglig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(df, bias_terms_weak_label, title_col=\"title\", desc_col=\"description\"):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each job description\n",
    "    for idx, row in df.iterrows():\n",
    "        title = row[title_col]\n",
    "        description = row[desc_col]\n",
    "\n",
    "        # Skip missing descriptions\n",
    "        if pd.isna(description) or description.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        # Correct: Split the WHOLE description into sentences\n",
    "        for sentence in sent_tokenize(description):\n",
    "            sentences.append(sentence)\n",
    "            # Weak labeling: check if any bias term is in the sentence\n",
    "            if any(bias_word in sentence.lower() for bias_word in bias_terms_weak_label):\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "    labeled_df = pd.DataFrame({'sentence': sentences, 'label': labels})\n",
    "    return labeled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = label_data(df, bias_terms_weak_label, title_col=\"title\", desc_col=\"description\")\n",
    "\n",
    "def classify_bias(df_labeled):\n",
    "    # Convert text to TF-IDF features\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(df_labeled['sentence'])\n",
    "    y = df_labeled['label']\n",
    "\n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    #Cross-validation of the classifier\n",
    "    #scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n",
    "    #cross_validation = {\"F1 scores:\", scores, \"Mean F1:\", scores.mean()}\n",
    "    return classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Bias Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95       769\n",
      "           1       1.00      0.03      0.05        76\n",
      "\n",
      "    accuracy                           0.91       845\n",
      "   macro avg       0.96      0.51      0.50       845\n",
      "weighted avg       0.92      0.91      0.87       845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Potential Bias Results:\")\n",
    "print(classify_bias(df_labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation of the classifier\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n",
    "print(\"F1 scores:\", scores)\n",
    "print(\"Mean F1:\", scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
